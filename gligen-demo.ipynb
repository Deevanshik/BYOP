{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Please Import the COCO dataset before running the code.","metadata":{}},{"cell_type":"code","source":"!pip install diffusers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:47:58.747417Z","iopub.execute_input":"2024-12-16T14:47:58.747894Z","iopub.status.idle":"2024-12-16T14:48:10.376287Z","shell.execute_reply.started":"2024-12-16T14:47:58.747827Z","shell.execute_reply":"2024-12-16T14:48:10.375438Z"}},"outputs":[{"name":"stdout","text":"Collecting diffusers\n  Downloading diffusers-0.31.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (7.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.26.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.4.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (10.3.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (2024.6.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.23.2->diffusers) (3.1.2)\nDownloading diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.31.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom diffusers import StableDiffusionGLIGENPipeline\nfrom diffusers.utils import load_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:48:10.378147Z","iopub.execute_input":"2024-12-16T14:48:10.378473Z","iopub.status.idle":"2024-12-16T14:48:27.541354Z","shell.execute_reply.started":"2024-12-16T14:48:10.378444Z","shell.execute_reply":"2024-12-16T14:48:27.540443Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d92f3181ce5f4d9b9373e3d142ddab78"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Insert the model in the pipeline\npipe = StableDiffusionGLIGENPipeline.from_pretrained(\n    \"masterful/gligen-1-4-inpainting-text-box\", variant=\"fp16\", torch_dtype=torch.float16\n)       \npipe = pipe.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:48:27.542631Z","iopub.execute_input":"2024-12-16T14:48:27.543288Z","iopub.status.idle":"2024-12-16T14:48:57.949302Z","shell.execute_reply.started":"2024-12-16T14:48:27.543248Z","shell.execute_reply":"2024-12-16T14:48:57.948579Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/543 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab081fd589cf41989ec166e036cda800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c38945dac14747a734bdaa8ed96de0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"safety_checker/config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc303b6b35a749c2a14d0624a8742f70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)kpoints/scheduler_config-checkpoint.json:   0%|          | 0.00/209 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dee08419a81b45beaaa7cc6344b86594"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91a29b0025514606892ac11950f93c62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler/scheduler_config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcec3517ed7d41aba0e93dfb510fddeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)ature_extractor/preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4f9b2faf16e4e7eba5f3edf77b181f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.fp16.bin:   0%|          | 0.00/608M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b9290a638634ebf9ae554d9e707135c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dee82a35314448f9643df9adc9b321d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f637099ff4fb4e029328e12e23f24d04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2828f6d6a43a4191986315f347d999f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/config.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7594c592fa44598cb3c532ff802eeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5b43765eb649f586be4806034d2c6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.fp16.bin:   0%|          | 0.00/2.14G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b883c8438f03437b97cf7da98b1957f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.fp16.bin:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08340a2791694a6d993a8448ad552aef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/config.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eada61a7bf048308e7d90926f9f1262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.fp16.bin:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffba2a4269de4be299576deb7f79e1c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"938ee960c2254838b50ba59ca43b59bd"}},"metadata":{}},{"name":"stderr","text":"An error occurred while trying to fetch /root/.cache/huggingface/hub/models--masterful--gligen-1-4-inpainting-text-box/snapshots/d6d957f8d27c40889c0d570a616571a5645c8be3/unet: Error no file named diffusion_pytorch_model.fp16.safetensors found in directory /root/.cache/huggingface/hub/models--masterful--gligen-1-4-inpainting-text-box/snapshots/d6d957f8d27c40889c0d570a616571a5645c8be3/unet.\nDefaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\nAn error occurred while trying to fetch /root/.cache/huggingface/hub/models--masterful--gligen-1-4-inpainting-text-box/snapshots/d6d957f8d27c40889c0d570a616571a5645c8be3/vae: Error no file named diffusion_pytorch_model.fp16.safetensors found in directory /root/.cache/huggingface/hub/models--masterful--gligen-1-4-inpainting-text-box/snapshots/d6d957f8d27c40889c0d570a616571a5645c8be3/vae.\nDefaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n/opt/conda/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nos.mkdir(\"/kaggle/working/output_images/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:48:57.950357Z","iopub.execute_input":"2024-12-16T14:48:57.950644Z","iopub.status.idle":"2024-12-16T14:48:57.954491Z","shell.execute_reply.started":"2024-12-16T14:48:57.950618Z","shell.execute_reply":"2024-12-16T14:48:57.953713Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport json\nfrom PIL import Image\n\n# Define paths to images and annotation file\nimages_path = \"/kaggle/input/coco-2017-dataset/coco2017/train2017\"\nannotations_file_path = \"/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_train2017.json\"\n\n# Load the annotations file\nwith open(annotations_file_path, 'r') as file:\n    annotations_data = json.load(file)\n\n# Mapping of image_id to its corresponding annotations\nimage_to_annotations = {}\nfor annotation in annotations_data['annotations']:\n    image_id = annotation['image_id']\n    if image_id not in image_to_annotations:\n        image_to_annotations[image_id] = []\n    image_to_annotations[image_id].append(annotation)\n\n# Mapping of category_id to category name\ncategory_id_to_name = {\n    category['id']: category['name'] for category in annotations_data['categories']\n}\n\n# Mapping of image file names to image metadata\nimage_name_to_metadata = {\n    image['file_name']: image for image in annotations_data['images']\n}\n\n# Iterate over all images in the dataset\nfor image_name in os.listdir(images_path):\n    # Load the image\n    image_path = os.path.join(images_path, image_name)\n    input_image = Image.open(image_path)\n\n    # Get the image metadata\n    if image_name not in image_name_to_metadata:\n        print(f\"Image {image_name} not found in annotations.\")\n        continue\n    \n    image_metadata = image_name_to_metadata[image_name]\n    image_id = image_metadata['id']\n\n    # Get the annotations for the current image\n    annotations = image_to_annotations.get(image_id, [])\n\n    # Extract objects and their bounding boxes\n    objects = []\n    boxes = []\n\n    for annotation in annotations:\n        category_id = annotation['category_id']\n        category_name = category_id_to_name[category_id]\n\n        # Add the object to the list\n        objects.append(f\"a {category_name}\")\n\n        # Normalize the bounding box coordinates\n        bbox = annotation['bbox']  # [x_min, y_min, width, height]\n        x_min, y_min, width, height = bbox\n        image_width, image_height = image_metadata['width'], image_metadata['height']\n\n        x_min_norm = x_min / image_width\n        y_min_norm = y_min / image_height\n        x_max_norm = (x_min + width) / image_width\n        y_max_norm = (y_min + height) / image_height\n\n        boxes.append([x_min_norm, y_min_norm, x_max_norm, y_max_norm])\n\n    # Generate the prompt and phrases\n    if len(objects) > 1:\n        prompt = \", \".join(objects[:-1]) + f\" and {objects[-1]}\"\n    else:\n        prompt = objects[0]\n\n    phrases = objects\n    \n    images = pipe(\n    prompt=prompt,\n    gligen_phrases=phrases,\n    gligen_inpaint_image=input_image,\n    gligen_boxes=boxes,\n    gligen_scheduled_sampling_beta=1,\n    output_type=\"pil\",\n    num_inference_steps=50,\n    ).images\n    images[0].save(f\"/kaggle/working/output_images/{image_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:49:11.205593Z","iopub.execute_input":"2024-12-16T14:49:11.206511Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py:747: FutureWarning: Accessing config attribute `sample_size` directly via 'AutoencoderKL' object attribute is deprecated. Please access 'sample_size' over 'AutoencoderKL's config object instead, e.g. 'unet.config.sample_size'.\n  if gligen_inpaint_image.size != (self.vae.sample_size, self.vae.sample_size):\n/opt/conda/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_gligen/pipeline_stable_diffusion_gligen.py:748: FutureWarning: Accessing config attribute `sample_size` directly via 'AutoencoderKL' object attribute is deprecated. Please access 'sample_size' over 'AutoencoderKL's config object instead, e.g. 'unet.config.sample_size'.\n  gligen_inpaint_image = self.target_size_center_crop(gligen_inpaint_image, self.vae.sample_size)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"302a01d208894aabbbf9588ff000808f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"271df4cd860743d0871dbd85b0665d70"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (92 > 77). Running this sequence through the model will result in indexing errors\nThe following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', a bicycle , a bicycle , a bicycle , a bicycle and a person']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea879cbb263d4deda668a257d97adcc0"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}